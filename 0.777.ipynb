{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 26.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of segments collected: 135\n",
            "‚úÖ Scaler Â∑≤ÂÑ≤Â≠òÂà∞Ôºö AICUP\\scaler.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [00:01<00:00, 79.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ÂÖ±ÂÑ≤Â≠ò 135 Á≠ÜÊ≠£Ë¶èÂåñÂæåÁöÑ (85,6) .npy Ëá≥ÔºöAICUP\\sequence_data_train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# === Ë®≠ÂÆöË≥áÊñôÂ§æ ===\n",
        "ROOT = Path('./AICUP')\n",
        "TXT_DIR = ROOT / 'train_data'              # Êàñ 'test_data'\n",
        "INFO_CSV = ROOT / 'train_info.csv'         # Êàñ 'test_info.csv'\n",
        "SAVE_DIR = ROOT / 'sequence_data_train'    # Êàñ 'sequence_data_test'\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === ËºâÂÖ• info.csv ‰∏¶ËôïÁêÜ cut_point ===\n",
        "info = pd.read_csv(INFO_CSV)\n",
        "\n",
        "def fix_cut_point_format(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    try:\n",
        "        return [int(n) for n in re.findall(r'\\d+', str(x))]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "info['cut_point'] = info['cut_point'].apply(fix_cut_point_format)\n",
        "info = info[info['cut_point'].apply(lambda x: len(x) >= 2)]  # Ëá≥Â∞ëËÉΩÂàáÂá∫ 1 ÊÆµ\n",
        "\n",
        "# === ËÆÄÂèñ txt Êàê numpy array ===\n",
        "def read_txt_as_array(txt_path):\n",
        "    lines = Path(txt_path).read_text().splitlines()\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 6:\n",
        "            try:\n",
        "                data.append([int(x) for x in parts])\n",
        "            except:\n",
        "                continue\n",
        "    return np.array(data)\n",
        "\n",
        "# === Êî∂ÈõÜÊâÄÊúâÊÆµËêΩÔºåÂæåÁ∫åÁµ±‰∏ÄÂª∫Á´ã scaler ===\n",
        "all_segments = []\n",
        "meta = []\n",
        "\n",
        "# === ÂàáÂâ≤ÊØèÂÄã txt Ê™îÊ°à ===\n",
        "for _, row in tqdm(info.iterrows(), total=len(info)):\n",
        "    uid = row['unique_id']\n",
        "    cut_points = row['cut_point']\n",
        "    txt_file = TXT_DIR / f\"{uid}.txt\"\n",
        "\n",
        "    if not txt_file.exists():\n",
        "        continue\n",
        "\n",
        "    raw = read_txt_as_array(txt_file)\n",
        "\n",
        "    for i in range(len(cut_points) - 1):\n",
        "        start, end = cut_points[i], cut_points[i + 1]\n",
        "        segment = raw[start:end]\n",
        "\n",
        "        if segment.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Ë£ú 0 Âà∞ (85, 6)\n",
        "        if segment.shape[0] < 85:\n",
        "            pad_len = 85 - segment.shape[0]\n",
        "            pad = np.zeros((pad_len, 6))\n",
        "            segment = np.vstack([segment, pad])\n",
        "        elif segment.shape[0] > 85:\n",
        "            segment = segment[:85]\n",
        "\n",
        "        if segment.shape != (85, 6):\n",
        "            continue\n",
        "\n",
        "        all_segments.append(segment)\n",
        "        meta.append((uid, i))\n",
        "\n",
        "# === Áµ±‰∏ÄÂª∫ scalerÔºåfit ÊâÄÊúâ segment ÂæåÂÑ≤Â≠ò ===\n",
        "print(f'Number of segments collected: {len(all_segments)}')\n",
        "all_data = np.concatenate(all_segments, axis=0)  # (N*85, 6)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(all_data)\n",
        "joblib.dump(scaler, ROOT / \"scaler.pkl\")\n",
        "print(\"‚úÖ Scaler Â∑≤ÂÑ≤Â≠òÂà∞Ôºö\", ROOT / \"scaler.pkl\")\n",
        "\n",
        "# === Ê≠£Ë¶èÂåñÂæåÂÜçÂÑ≤Â≠òÊØèÁ≠Ü segment ===\n",
        "for segment, (uid, i) in tqdm(zip(all_segments, meta), total=len(all_segments)):\n",
        "    normed = scaler.transform(segment)\n",
        "    np.save(SAVE_DIR / f\"{uid}_{i}.npy\", normed)\n",
        "\n",
        "print(f\"‚úÖ ÂÖ±ÂÑ≤Â≠ò {len(all_segments)} Á≠ÜÊ≠£Ë¶èÂåñÂæåÁöÑ (85,6) .npy Ëá≥Ôºö{SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[‚úÖ Ê™îÊ°àÊâæÂà∞] AICUP\\test_data\\1968.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à:  20%|‚ñà‚ñà        | 1/5 [00:00<00:01,  3.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[‚úÖ Ê™îÊ°àÊâæÂà∞] AICUP\\test_data\\1969.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:01,  2.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[‚úÖ Ê™îÊ°àÊâæÂà∞] AICUP\\test_data\\1970.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:01<00:01,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[‚úÖ Ê™îÊ°àÊâæÂà∞] AICUP\\test_data\\1971.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:03<00:00,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[‚úÖ Ê™îÊ°àÊâæÂà∞] AICUP\\test_data\\1972.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ ÂÖ±ÂÑ≤Â≠ò 135 Á≠ÜÊ∏¨Ë©¶Ë≥áÊñôÂà∞ÔºöAICUP\\sequence_data_test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "# === Ë∑ØÂæëË®≠ÂÆö ===\n",
        "ROOT = Path('./AICUP')\n",
        "TXT_DIR = ROOT / 'test_data'\n",
        "INFO_CSV = ROOT / 'test_info.csv'\n",
        "SAVE_DIR = ROOT / 'sequence_data_test'\n",
        "SCALER_PATH = ROOT / 'scaler.pkl'\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === ËºâÂÖ• cut_point ===\n",
        "info = pd.read_csv(INFO_CSV)\n",
        "\n",
        "def fix_cut_point_format(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    return [int(n) for n in re.findall(r'\\d+', str(x))]\n",
        "\n",
        "info['cut_point'] = info['cut_point'].apply(fix_cut_point_format)\n",
        "info = info[info['cut_point'].apply(lambda x: len(x) >= 2)]\n",
        "\n",
        "# === ËºâÂÖ•Ë®ìÁ∑¥Áî®ÁöÑ StandardScaler ===\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "\n",
        "# === txt to array ===\n",
        "def read_txt_as_array(txt_path):\n",
        "    lines = Path(txt_path).read_text().splitlines()\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 6:\n",
        "            try:\n",
        "                data.append([int(x) for x in parts])\n",
        "            except:\n",
        "                continue\n",
        "    return np.array(data)\n",
        "\n",
        "# === ÈñãÂßãËôïÁêÜÊØèÂÄãÊ™îÊ°à ===\n",
        "count = 0\n",
        "for _, row in tqdm(info.iterrows(), total=len(info), desc=\"üìÇ ËôïÁêÜÊ∏¨Ë©¶Ê™îÊ°à\"):\n",
        "    uid = row['unique_id']\n",
        "    cut_points = row['cut_point']\n",
        "    uid_int = int(float(uid))\n",
        "    txt_file = TXT_DIR / f\"{uid_int}.txt\"\n",
        "    \n",
        "    if not txt_file.exists():\n",
        "        print(f\"[‚ùå Ê™îÊ°à‰∏çÂ≠òÂú®] {txt_file}\")\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"[‚úÖ Ê™îÊ°àÊâæÂà∞] {txt_file}\")\n",
        "\n",
        "\n",
        "    if not txt_file.exists():\n",
        "        continue\n",
        "\n",
        "    raw = read_txt_as_array(txt_file)\n",
        "\n",
        "    for i in range(len(cut_points) - 1):\n",
        "        start, end = cut_points[i], cut_points[i + 1]\n",
        "        segment = raw[start:end]\n",
        "\n",
        "        # Ë£ú 0 Âà∞ (85, 6)\n",
        "        if segment.shape[0] < 85:\n",
        "            pad = np.zeros((85 - segment.shape[0], 6))\n",
        "            segment = np.vstack([segment, pad])\n",
        "        elif segment.shape[0] > 85:\n",
        "            segment = segment[:85]\n",
        "\n",
        "        if segment.shape != (85, 6):\n",
        "            continue\n",
        "\n",
        "        # ‰ΩøÁî® scaler ÂÅö Z-score Ê≠£Ë¶èÂåñ\n",
        "        normed = scaler.transform(segment)\n",
        "\n",
        "        # ÂÑ≤Â≠òÁÇ∫ npy\n",
        "        np.save(SAVE_DIR / f\"{uid}_{i}.npy\", normed)\n",
        "        count += 1\n",
        "\n",
        "print(f\"\\n‚úÖ ÂÖ±ÂÑ≤Â≠ò {count} Á≠ÜÊ∏¨Ë©¶Ë≥áÊñôÂà∞Ôºö{SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ËÆÄÂèñ npy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.49it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÊ®ôÁ±§Ê™îÊ°àË∑ØÂæë ===\n",
        "DATA_DIR = Path(\"./AICUP/sequence_data_train\")\n",
        "INFO_CSV = Path(\"./AICUP/train_info.csv\")\n",
        "\n",
        "# === ËÆÄÂèñ train_info.csv ===\n",
        "info = pd.read_csv(INFO_CSV)\n",
        "\n",
        "# === Êî∂ÈõÜÊØèÁ≠Ü npy Ë≥áÊñôËàáÊ®ôÁ±§ ===\n",
        "from tqdm import tqdm\n",
        "\n",
        "X, y_gender, y_handed, y_years, y_level, groups = [], [], [], [], [], []\n",
        "\n",
        "for i, row in tqdm(info.iterrows(), total=len(info), desc=\"ËÆÄÂèñ npy\"):\n",
        "    uid = row['unique_id']\n",
        "    pid = row['player_id']\n",
        "    for seg_id in range(27):\n",
        "        npy_path = DATA_DIR / f\"{uid}_{seg_id}.npy\"\n",
        "        if not npy_path.exists():\n",
        "            continue\n",
        "        data = np.load(npy_path)\n",
        "        if data.shape != (85, 6):\n",
        "            continue\n",
        "        X.append(data)\n",
        "        y_gender.append(row['gender'])\n",
        "        y_handed.append(row['hold racket handed'])\n",
        "        y_years.append(row['play years'])\n",
        "        y_level.append(row['level'])\n",
        "        groups.append(pid)\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# === Ê®ôÁ±§Á∑®Á¢º ===\n",
        "le_gender = LabelEncoder(); y_gender = le_gender.fit_transform(y_gender)\n",
        "le_handed = LabelEncoder(); y_handed = le_handed.fit_transform(y_handed)\n",
        "le_years = LabelEncoder(); y_years = le_years.fit_transform(y_years)\n",
        "le_level = LabelEncoder(); y_level = le_level.fit_transform(y_level)\n",
        "\n",
        "# === ÂàÜË®ìÁ∑¥/È©óË≠âÈõÜÔºà‰æù player_idÔºâ===\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(X, groups=groups))\n",
        "\n",
        "X_train, X_val = X[train_idx], X[val_idx]\n",
        "y_gender_train, y_gender_val = y_gender[train_idx], y_gender[val_idx]\n",
        "y_handed_train, y_handed_val = y_handed[train_idx], y_handed[val_idx]\n",
        "y_years_train, y_years_val = y_years[train_idx], y_years[val_idx]\n",
        "y_level_train, y_level_val = y_level[train_idx], y_level[val_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def FFT(xreal, ximag):    \n",
        "    n = 2\n",
        "    while(n*2 <= len(xreal)):\n",
        "        n *= 2\n",
        "    \n",
        "    p = int(math.log(n, 2))\n",
        "    \n",
        "    for i in range(0, n):\n",
        "        a = i\n",
        "        b = 0\n",
        "        for j in range(0, p):\n",
        "            b = int(b*2 + a%2)\n",
        "            a = a/2\n",
        "        if(b > i):\n",
        "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
        "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
        "            \n",
        "    wreal = []\n",
        "    wimag = []\n",
        "        \n",
        "    arg = float(-2 * math.pi / n)\n",
        "    treal = float(math.cos(arg))\n",
        "    timag = float(math.sin(arg))\n",
        "    \n",
        "    wreal.append(float(1.0))\n",
        "    wimag.append(float(0.0))\n",
        "    \n",
        "    for j in range(1, int(n/2)):\n",
        "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
        "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
        "        \n",
        "    m = 2\n",
        "    while(m < n + 1):\n",
        "        for k in range(0, n, m):\n",
        "            for j in range(0, int(m/2), 1):\n",
        "                index1 = k + j\n",
        "                index2 = int(index1 + m / 2)\n",
        "                t = int(n * j / m)\n",
        "                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
        "                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
        "                ureal = xreal[index1]\n",
        "                uimag = ximag[index1]\n",
        "                xreal[index1] = ureal + treal\n",
        "                ximag[index1] = uimag + timag\n",
        "                xreal[index2] = ureal - treal\n",
        "                ximag[index2] = uimag - timag\n",
        "        m *= 2\n",
        "        \n",
        "    return n, xreal, ximag   \n",
        "    \n",
        "def FFT_data(input_data, swinging_times):   \n",
        "    txtlength = swinging_times[-1] - swinging_times[0]\n",
        "    a_mean = [0] * txtlength\n",
        "    g_mean = [0] * txtlength\n",
        "       \n",
        "    for num in range(len(swinging_times)-1):\n",
        "        a = []\n",
        "        g = []\n",
        "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
        "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
        "\n",
        "        a_mean[num] = (sum(a) / len(a))\n",
        "        g_mean[num] = (sum(a) / len(a))\n",
        "    \n",
        "    return a_mean, g_mean\n",
        "\n",
        "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
        "    allsum = []\n",
        "    mean = []\n",
        "    var = []\n",
        "    rms = []\n",
        "    XYZmean_a = 0\n",
        "    a = []\n",
        "    g = []\n",
        "    a_s1 = 0\n",
        "    a_s2 = 0\n",
        "    g_s1 = 0\n",
        "    g_s2 = 0\n",
        "    a_k1 = 0\n",
        "    a_k2 = 0\n",
        "    g_k1 = 0\n",
        "    g_k2 = 0\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            allsum = input_data[i]\n",
        "            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "            continue\n",
        "        \n",
        "        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "       \n",
        "        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            var = input_data[i]\n",
        "            rms = input_data[i]\n",
        "            continue\n",
        "\n",
        "        var = [var[feature_index] + math.pow((input_data[i][feature_index] - mean[feature_index]), 2) for feature_index in range(len(input_data[i]))]\n",
        "        rms = [rms[feature_index] + math.pow(input_data[i][feature_index], 2) for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    a_max = [max(a)]\n",
        "    a_min = [min(a)]\n",
        "    a_mean = [sum(a) / len(a)]\n",
        "    g_max = [max(g)]\n",
        "    g_min = [min(g)]\n",
        "    g_mean = [sum(g) / len(g)]\n",
        "    \n",
        "    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n",
        "        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n",
        "        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n",
        "        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n",
        "        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n",
        "        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n",
        "    \n",
        "    a_s1 = a_s1 / len(input_data)\n",
        "    a_s2 = a_s2 / len(input_data)\n",
        "    g_s1 = g_s1 / len(input_data)\n",
        "    g_s2 = g_s2 / len(input_data)\n",
        "    a_k2 = math.pow(a_s2, 1.5)\n",
        "    g_k2 = math.pow(g_s2, 1.5)\n",
        "    a_s2 = a_s2 * a_s2\n",
        "    g_s2 = g_s2 * g_s2\n",
        "    \n",
        "    a_kurtosis = [a_s1 / a_s2]\n",
        "    g_kurtosis = [g_s1 / g_s2]\n",
        "    a_skewness = [a_k1 / a_k2]\n",
        "    g_skewness = [g_k1 / g_k2]\n",
        "    \n",
        "    a_fft_mean = 0\n",
        "    g_fft_mean = 0\n",
        "    cut = int(n_fft / swinging_times)\n",
        "    a_psd = []\n",
        "    g_psd = []\n",
        "    entropy_a = []\n",
        "    entropy_g = []\n",
        "    e1 = []\n",
        "    e3 = []\n",
        "    e2 = 0\n",
        "    e4 = 0\n",
        "    \n",
        "    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n",
        "        a_fft_mean += a_fft[i]\n",
        "        g_fft_mean += g_fft[i]\n",
        "        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n",
        "        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n",
        "        e1.append(math.pow(a_psd[-1], 0.5))\n",
        "        e3.append(math.pow(g_psd[-1], 0.5))\n",
        "        \n",
        "    a_fft_mean = a_fft_mean / cut\n",
        "    g_fft_mean = g_fft_mean / cut\n",
        "    \n",
        "    a_psd_mean = sum(a_psd) / len(a_psd)\n",
        "    g_psd_mean = sum(g_psd) / len(g_psd)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        e2 += math.pow(a_psd[i], 0.5)\n",
        "        e4 += math.pow(g_psd[i], 0.5)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n",
        "        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n",
        "    \n",
        "    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n",
        "    g_entropy_mean = sum(entropy_g) / len(entropy_g)       \n",
        "        \n",
        "    \n",
        "    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n",
        "    writer.writerow(output)\n",
        "\n",
        "def data_generate():\n",
        "    datapath = './AICUP/train_data'\n",
        "    tar_dir = './AICUP/tabular_data_train'\n",
        "    pathlist_txt = Path(datapath).glob('**/*.txt')\n",
        "    os.makedirs(tar_dir, exist_ok=True)\n",
        "    \n",
        "    for file in pathlist_txt:\n",
        "        f = open(file)\n",
        "\n",
        "        All_data = []\n",
        "\n",
        "        count = 0\n",
        "        for line in f.readlines():\n",
        "            if line == '\\n' or count == 0:\n",
        "                count += 1\n",
        "                continue\n",
        "            num = line.split(' ')\n",
        "            if len(num) > 5:\n",
        "                tmp_list = []\n",
        "                for i in range(6):\n",
        "                    tmp_list.append(int(num[i]))\n",
        "                All_data.append(tmp_list)\n",
        "        \n",
        "        f.close()\n",
        "\n",
        "        swing_index = np.linspace(0, len(All_data), 28, dtype = int)\n",
        "        # filename.append(int(Path(file).stem))\n",
        "        # all_swing.append([swing_index])\n",
        "\n",
        "        headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']                \n",
        "        \n",
        "\n",
        "        with open('./{dir}/{fname}.csv'.format(dir = tar_dir, fname = Path(file).stem), 'w', newline = '') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(headerList)\n",
        "            try:\n",
        "                a_fft, g_fft = FFT_data(All_data, swing_index)\n",
        "                a_fft_imag = [0] * len(a_fft)\n",
        "                g_fft_imag = [0] * len(g_fft)\n",
        "                n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
        "                n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
        "                for i in range(len(swing_index)):\n",
        "                    if i==0:\n",
        "                        continue\n",
        "                    feature(All_data[swing_index[i-1]: swing_index[i]], i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
        "            except:\n",
        "                print(Path(file).stem)\n",
        "                continue\n",
        "data_generate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tabular MinMaxScaler Â∑≤ÂÑ≤Â≠òËá≥: ./AICUP/tabular_scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "\n",
        "# === Ë®≠ÂÆöË∑ØÂæë ===\n",
        "TABULAR_DIR = Path(\"./AICUP/tabular_data_train\")\n",
        "SCALER_PATH = \"./AICUP/tabular_scaler.pkl\"\n",
        "\n",
        "# === Êî∂ÈõÜÊâÄÊúâ tabular ÁâπÂæµ ===\n",
        "tabular_list = []\n",
        "for file in sorted(TABULAR_DIR.glob(\"*.csv\")):\n",
        "    df = pd.read_csv(file)\n",
        "    tabular_list.append(df.values)  # shape: (27, 34)\n",
        "\n",
        "X_all = np.vstack(tabular_list)  # shape: (N, 34)\n",
        "\n",
        "# === Âª∫Á´ãËàáÂÑ≤Â≠ò MinMaxScaler ===\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_all)\n",
        "\n",
        "joblib.dump(scaler, SCALER_PATH)\n",
        "print(f\"‚úÖ Tabular MinMaxScaler Â∑≤ÂÑ≤Â≠òËá≥: {SCALER_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 40, 6)]              0         []                            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 40, 64)               1216      ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 40, 64)               256       ['conv1d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 40, 128)              41088     ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 40, 128)              512       ['conv1d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)        [(None, 34)]                 0         []                            \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPoolin  (None, 20, 128)              0         ['batch_normalization_7[0][0]'\n",
            " g1D)                                                               ]                             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 256)                  8960      ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 20, 128)              0         ['max_pooling1d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 256)                  1024      ['dense_4[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  (None, 20, 256)              263168    ['dropout_7[0][0]']           \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 256)                  0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 20, 256)              512       ['bidirectional_2[0][0]']     \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 128)                  32896     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 20, 256)              131712    ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 128)                  512       ['dense_5[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 20, 256)              0         ['layer_normalization_2[0][0]'\n",
            " OpLambda)                                                          , 'multi_head_attention_1[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 128)                  0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 20, 256)              512       ['tf.__operators__.add_1[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 64)                   8256      ['dropout_9[0][0]']           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 256)                  0         ['layer_normalization_3[0][0]'\n",
            "  (GlobalAveragePooling1D)                                          ]                             \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 64)                   0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 320)                  0         ['global_average_pooling1d_1[0\n",
            " )                                                                  ][0]',                        \n",
            "                                                                     'dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 128)                  41088     ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 128)                  0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " gender (Dense)              (None, 1)                    129       ['dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            " handed (Dense)              (None, 1)                    129       ['dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            " years (Dense)               (None, 3)                    387       ['dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            " level (Dense)               (None, 4)                    516       ['dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 532873 (2.03 MB)\n",
            "Trainable params: 531721 (2.03 MB)\n",
            "Non-trainable params: 1152 (4.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 16s 1s/step - loss: 2.9569 - gender_loss: 0.3257 - handed_loss: 0.2390 - years_loss: 0.7389 - level_loss: 1.6534 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 0.8490 - level_auc: 0.6509 - val_loss: 0.8161 - val_gender_loss: 0.3907 - val_handed_loss: 0.0845 - val_years_loss: 0.1020 - val_level_loss: 0.2389 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 1s 259ms/step - loss: 0.3178 - gender_loss: 0.1116 - handed_loss: 0.0362 - years_loss: 0.0410 - level_loss: 0.1290 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 0.9916 - val_loss: 0.1482 - val_gender_loss: 0.0924 - val_handed_loss: 0.0178 - val_years_loss: 0.0134 - val_level_loss: 0.0245 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 1s 225ms/step - loss: 0.0584 - gender_loss: 0.0213 - handed_loss: 0.0064 - years_loss: 0.0079 - level_loss: 0.0228 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000 - val_loss: 0.0191 - val_gender_loss: 0.0115 - val_handed_loss: 0.0027 - val_years_loss: 0.0022 - val_level_loss: 0.0027 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0045 - gender_loss: 0.0018 - handed_loss: 0.0011 - years_loss: 8.1552e-04 - level_loss: 7.6959e-04 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.0045 - gender_loss: 0.0018 - handed_loss: 0.0011 - years_loss: 8.1552e-04 - level_loss: 7.6959e-04 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000 - val_loss: 0.0032 - val_gender_loss: 0.0017 - val_handed_loss: 5.5813e-04 - val_years_loss: 4.7410e-04 - val_level_loss: 4.8170e-04 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0031 - gender_loss: 9.0911e-04 - handed_loss: 5.4344e-04 - years_loss: 9.7601e-04 - level_loss: 6.8915e-04 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000 - val_loss: 0.0019 - val_gender_loss: 8.9400e-04 - val_handed_loss: 3.7755e-04 - val_years_loss: 2.9454e-04 - val_level_loss: 3.0526e-04 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0023 - gender_loss: 3.7992e-04 - handed_loss: 3.5639e-04 - years_loss: 5.1742e-04 - level_loss: 0.0010 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000 - val_loss: 0.0014 - val_gender_loss: 6.2206e-04 - val_handed_loss: 3.3489e-04 - val_years_loss: 2.2532e-04 - val_level_loss: 2.4412e-04 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0012 - gender_loss: 2.5858e-04 - handed_loss: 3.6277e-04 - years_loss: 3.0666e-04 - level_loss: 2.9627e-04 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "4/4 [==============================] - 1s 199ms/step - loss: 0.0012 - gender_loss: 2.5858e-04 - handed_loss: 3.6277e-04 - years_loss: 3.0666e-04 - level_loss: 2.9627e-04 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000 - val_loss: 0.0014 - val_gender_loss: 5.5272e-04 - val_handed_loss: 4.1929e-04 - val_years_loss: 2.0387e-04 - val_level_loss: 2.3855e-04 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0012 - gender_loss: 3.2135e-04 - handed_loss: 2.6466e-04 - years_loss: 3.5807e-04 - level_loss: 2.7459e-04 - gender_auc: 0.0000e+00 - handed_auc: 0.0000e+00 - years_auc: 1.0000 - level_auc: 1.0000 - val_loss: 0.0025 - val_gender_loss: 8.2393e-04 - val_handed_loss: 0.0010 - val_years_loss: 2.6024e-04 - val_level_loss: 3.4860e-04 - val_gender_auc: 0.0000e+00 - val_handed_auc: 0.0000e+00 - val_years_auc: 1.0000 - val_level_auc: 1.0000 - lr: 2.5000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2ade0c8be80>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Dropout, Bidirectional, LSTM, GlobalAveragePooling1D, Dense, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import AUC\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "\n",
        "# === ÂèÉÊï∏Ë®≠ÂÆö ===\n",
        "SEQ_DIR = Path('./AICUP/sequence_data_train')\n",
        "TAB_DIR = Path('./AICUP/tabular_data_train')\n",
        "INFO_CSV = './AICUP/train_info.csv'\n",
        "WINDOW = 40\n",
        "STRIDE = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# === ËÆÄÂèñË®ìÁ∑¥Ë≥áË®ä ===\n",
        "info = pd.read_csv(INFO_CSV).set_index(\"unique_id\")\n",
        "\n",
        "# === Áî¢Áîü sliding window ÂàáÁâáÊ∏ÖÂñÆ ===\n",
        "samples = []\n",
        "for file in sorted(SEQ_DIR.glob(\"*.npy\")):\n",
        "    uid, seg_id = file.stem.split(\"_\")\n",
        "    uid, seg_id = int(uid), int(seg_id)\n",
        "    if uid not in info.index: continue\n",
        "    samples.append({'uid': uid, 'seg_id': seg_id, 'seq_path': file, 'tab_path': TAB_DIR / f\"{uid}.csv\"})\n",
        "\n",
        "samples_df = pd.DataFrame(samples)\n",
        "\n",
        "# === ÂàÜÁæ§Âàá train/val ===\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(samples_df, groups=samples_df['uid']))\n",
        "train_samples = samples_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_samples = samples_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "# === tabular scaler ===\n",
        "tab_all = []\n",
        "for s in samples:\n",
        "    tab = pd.read_csv(s['tab_path']).values\n",
        "    tab_all.append(tab)\n",
        "tab_all = np.vstack(tab_all)\n",
        "scaler = StandardScaler().fit(tab_all)\n",
        "joblib.dump(scaler, './tabular_scaler.pkl')\n",
        "\n",
        "# === label encoding ===\n",
        "label_encoders = {}\n",
        "for col in ['gender', 'hold racket handed', 'play years', 'level']:\n",
        "    le = LabelEncoder()\n",
        "    info[col] = le.fit_transform(info[col])\n",
        "    label_encoders[col] = le\n",
        "joblib.dump(label_encoders, './label_encoders.pkl')\n",
        "\n",
        "# === Ë≥áÊñôÁî¢ÁîüÂô® ===\n",
        "class DualInputGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size, scaler, info_df):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.scaler = scaler\n",
        "        self.info_df = info_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_seq, X_tab, y_gender, y_handed, y_years, y_level = [], [], [], [], [], []\n",
        "\n",
        "        for _, row in batch.iterrows():\n",
        "            seq = np.load(row['seq_path'])\n",
        "            if seq.shape[0] < WINDOW:\n",
        "                pad = np.zeros((WINDOW - seq.shape[0], 6))\n",
        "                seq = np.vstack([seq, pad])\n",
        "            elif seq.shape[0] > WINDOW:\n",
        "                seq = seq[:WINDOW]\n",
        "\n",
        "            tab = pd.read_csv(row['tab_path']).iloc[row['seg_id']].values.astype(np.float32)\n",
        "            tab = self.scaler.transform([tab])[0]\n",
        "\n",
        "            label_row = self.info_df.loc[row['uid']]\n",
        "            yg, yh = label_row['gender'], label_row['hold racket handed']\n",
        "            yy, yl = label_row['play years'], label_row['level']\n",
        "\n",
        "            X_seq.append(seq)\n",
        "            X_tab.append(tab)\n",
        "            y_gender.append(yg)\n",
        "            y_handed.append(yh)\n",
        "            y_years.append(yy)\n",
        "            y_level.append(yl)\n",
        "\n",
        "        return (\n",
        "            [np.array(X_seq), np.array(X_tab)],\n",
        "            {\n",
        "                'gender': np.array(y_gender).astype(np.float32),\n",
        "                'handed': np.array(y_handed).astype(np.float32),\n",
        "                'years': to_categorical(y_years, num_classes=3),\n",
        "                'level': to_categorical(y_level, num_classes=4),\n",
        "            }\n",
        "        )\n",
        "\n",
        "train_gen = DualInputGenerator(train_samples, BATCH_SIZE, scaler, info)\n",
        "val_gen = DualInputGenerator(val_samples, BATCH_SIZE, scaler, info)\n",
        "\n",
        "# === Âª∫Á´ãÊ®°Âûã ===\n",
        "seq_input = Input(shape=(40, 6))\n",
        "x = Conv1D(64, 3, activation='relu', padding='same')(seq_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "x = LayerNormalization()(x)\n",
        "attn2 = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "x = LayerNormalization()(x + attn2)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "# Âº∑Âåñ Tabular ÂàÜÊîØ\n",
        "tab_input = Input(shape=(34,))\n",
        "t = Dense(256, activation='relu')(tab_input)\n",
        "t = BatchNormalization()(t)\n",
        "t = Dropout(0.4)(t)\n",
        "t = Dense(128, activation='relu')(t)\n",
        "t = BatchNormalization()(t)\n",
        "t = Dropout(0.4)(t)\n",
        "t = Dense(64, activation='relu')(t)\n",
        "t = Dropout(0.4)(t)\n",
        "\n",
        "merged = concatenate([x, t])\n",
        "merged = Dense(128, activation='relu')(merged)\n",
        "merged = Dropout(0.4)(merged)\n",
        "\n",
        "output_gender = Dense(1, activation='sigmoid', name='gender')(merged)\n",
        "output_handed = Dense(1, activation='sigmoid', name='handed')(merged)\n",
        "output_years = Dense(3, activation='softmax', name='years')(merged)\n",
        "output_level = Dense(4, activation='softmax', name='level')(merged)\n",
        "\n",
        "model = Model(inputs=[seq_input, tab_input], outputs=[output_gender, output_handed, output_years, output_level])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss={\n",
        "        'gender': 'binary_crossentropy',\n",
        "        'handed': 'binary_crossentropy',\n",
        "        'years': 'categorical_crossentropy',\n",
        "        'level': 'categorical_crossentropy'\n",
        "    },\n",
        "    metrics={\n",
        "        'gender': AUC(name='auc'),\n",
        "        'handed': AUC(name='auc'),\n",
        "        'years': AUC(name='auc'),\n",
        "        'level': AUC(name='auc')\n",
        "    }\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# === Ë®ìÁ∑¥ ===\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_level_auc', patience=7, mode='max', restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_level_auc', factor=0.5, patience=3, verbose=1, mode='max')\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òËá≥ ./AICUP/model_fusion.h5\n"
          ]
        }
      ],
      "source": [
        "# ÂÑ≤Â≠òÊ®°ÂûãÔºàÂª∫Ë≠∞Ê†ºÂºèÔºâ\n",
        "model.save(\"./AICUP/model_fusion.keras\")\n",
        "print(\"‚úÖ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òËá≥ ./AICUP/model_fusion.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üß† Êé®Ë´ñ‰∏≠: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Â∑≤ÂÑ≤Â≠òÈ†êÊ∏¨ÁµêÊûúËá≥: ./AICUP/sample_submission.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Ë∑ØÂæëË®≠ÂÆö ===\n",
        "MODEL_PATH = \"./AICUP/model_fusion.keras\"\n",
        "SCALER_PATH = \"./tabular_scaler.pkl\"\n",
        "SEQ_DIR = Path(\"./AICUP/sequence_data_test\")\n",
        "TAB_DIR = Path(\"./AICUP/tabular_data_test\")\n",
        "SUBMIT_PATH = \"./AICUP/sample_submission.csv\"\n",
        "WINDOW = 40\n",
        "\n",
        "# === ËºâÂÖ•Ê®°ÂûãËàá scaler ===\n",
        "model = load_model(MODEL_PATH, compile=False)\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "# === Ë£úÁ©∫ÁôΩÁöÑÂ£ûÊéâÊ™îÊ°à ===\n",
        "# for f in invalid_files:\n",
        "#     np.save(f, np.zeros((85, 6), dtype=np.float32))\n",
        "# === Êï¥ÁêÜÊØè‰ΩçÈÅ∏ÊâãÊâÄÊúâÊÆµËêΩ ===\n",
        "uid_dict = defaultdict(list)\n",
        "for file in sorted(SEQ_DIR.glob(\"*.npy\")):\n",
        "    uid_str = \"_\".join(file.stem.split(\"_\")[:-1])  # e.g. '1968.0'\n",
        "    uid = int(float(uid_str))  # ÂÖà float ÂÜç int\n",
        "    uid_dict[uid].append(file)\n",
        "\n",
        "submit_rows = []\n",
        "\n",
        "for uid in tqdm(sorted(uid_dict.keys()), desc=\"üß† Êé®Ë´ñ‰∏≠\"):\n",
        "    segments_seq, segments_tab = [], []\n",
        "\n",
        "    for file in sorted(uid_dict[uid]):\n",
        "        seg_id = int(file.stem.split(\"_\")[-1])\n",
        "        seq = np.load(file, allow_pickle=True)\n",
        "        if seq.shape[0] < WINDOW:\n",
        "            pad = np.zeros((WINDOW - seq.shape[0], 6))\n",
        "            seq = np.vstack([seq, pad])\n",
        "        elif seq.shape[0] > WINDOW:\n",
        "            seq = seq[:WINDOW]\n",
        "        segments_seq.append(seq)\n",
        "\n",
        "        tab_path = TAB_DIR / f\"{uid}.csv\"\n",
        "        if not tab_path.exists():\n",
        "            print(f\"‚ö†Ô∏è Êâæ‰∏çÂà∞ tabular Ë≥áÊñô: {tab_path}\")\n",
        "            continue\n",
        "        tab_df = pd.read_csv(tab_path)\n",
        "        if seg_id >= len(tab_df):\n",
        "            print(f\"‚ö†Ô∏è Ë∑≥ÈÅé {uid}_{seg_id}Ôºåtabular ÁÑ°Â∞çÊáâË≥áÊñô\")\n",
        "            continue\n",
        "        tab = tab_df.iloc[seg_id].values.astype(np.float32)\n",
        "        tab = scaler.transform([tab])[0]\n",
        "        segments_tab.append(tab)\n",
        "\n",
        "    if not segments_seq or len(segments_seq) != len(segments_tab):\n",
        "        print(f\"‚ö†Ô∏è UID {uid} ÁöÑÊÆµÊï∏‰∏ç‰∏ÄËá¥ÔºåË∑≥ÈÅé\")\n",
        "        continue\n",
        "\n",
        "    X_seq = np.array(segments_seq)\n",
        "    X_tab = np.array(segments_tab)\n",
        "\n",
        "    preds = model.predict([X_seq, X_tab], verbose=0)\n",
        "    if not isinstance(preds, list):\n",
        "        preds = [preds]\n",
        "\n",
        "    weighted_preds = []\n",
        "    for i, p in enumerate(preds):\n",
        "        if p.shape[1] == 1:\n",
        "            # sigmoidÔºöÂÖà squeezeÔºåÂÜçÂèçËΩâ ‚Üí È†êÊ∏¨ label=1 ÁöÑÊ©üÁéáÔºàÁî∑/Âè≥ÊâãÔºâ\n",
        "            p = p.squeeze(axis=1)\n",
        "            p = 1.0 - p  # üëà ÂèçËΩâÔºöÂéüÊú¨ÊòØ label=0ÔºàÂ•≥/Â∑¶ÊâãÔºâÁöÑÊ©üÁéá\n",
        "            weights = p\n",
        "            weighted_avg = np.average(p, weights=weights)\n",
        "            weighted_preds.append(weighted_avg)\n",
        "        else:\n",
        "            # softmax\n",
        "            weights = np.max(p, axis=1)\n",
        "            weighted_avg = np.average(p, axis=0, weights=weights)\n",
        "            weighted_preds.append(weighted_avg)\n",
        "\n",
        "    row = [\n",
        "        uid,\n",
        "        np.float32(weighted_preds[0]),         # gender ‚Üí Áî∑ÁîüÊ©üÁéá\n",
        "        np.float32(weighted_preds[1]),         # handed ‚Üí Âè≥ÊâãÊ©üÁéá\n",
        "        *map(np.float32, weighted_preds[2]),   # play years softmax: 3 È°û\n",
        "        *map(np.float32, weighted_preds[3])    # level softmax: 4 È°û\n",
        "    ]\n",
        "    submit_rows.append(row)\n",
        "\n",
        "# === Ëº∏Âá∫ CSV ===\n",
        "columns = [\n",
        "    \"unique_id\", \"gender\", \"hold racket handed\",\n",
        "    \"play years_0\", \"play years_1\", \"play years_2\",\n",
        "    \"level_2\", \"level_3\", \"level_4\", \"level_5\"\n",
        "]\n",
        "df_submit = pd.DataFrame(submit_rows, columns=columns)\n",
        "df_submit = df_submit.sort_values(\"unique_id\")\n",
        "float_cols = df_submit.columns.difference([\"unique_id\"])\n",
        "df_submit[float_cols] = df_submit[float_cols].astype(np.float32)\n",
        "df_submit.to_csv(SUBMIT_PATH, index=False, float_format=\"%.4f\")\n",
        "\n",
        "print(f\"\\n‚úÖ Â∑≤ÂÑ≤Â≠òÈ†êÊ∏¨ÁµêÊûúËá≥: {SUBMIT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ÂÖ±ÊâæÂà∞ 0 ÂÄãÁÑ°Ê≥ïËÆÄÂèñÁöÑÊ™îÊ°à\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "invalid_files = []\n",
        "\n",
        "for file in sorted(Path(\"AICUP/sequence_data_test\").glob(\"*.npy\")):\n",
        "    try:\n",
        "        data = np.load(file, allow_pickle=False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ÁÑ°Ê≥ïËÆÄÂèñÔºö{file.name}ÔºåÂéüÂõ†Ôºö{e}\")\n",
        "        invalid_files.append(file)\n",
        "\n",
        "print(f\"\\nÂÖ±ÊâæÂà∞ {len(invalid_files)} ÂÄãÁÑ°Ê≥ïËÆÄÂèñÁöÑÊ™îÊ°à\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ÊâæÂà∞Ê™îÊ°àÔºö./AICUP/sample_submission.csv\n",
            "‚úÖ Ê¨Ñ‰ΩçÂêçÁ®±Ê≠£Á¢∫\n",
            "‚úÖ Ê¨Ñ‰ΩçÊï∏ÈáèÊ≠£Á¢∫\n",
            "‚úÖ ÊâÄÊúâÊ©üÁéáÊ¨Ñ‰ΩçÁöÜÁÇ∫Êï∏ÂÄºÂûã\n",
            "‚úÖ ÊâÄÊúâÈ†êÊ∏¨Ê©üÁéáÈÉΩÂú® 0 ~ 1 ÁØÑÂúçÂÖß\n",
            "üéâ CSV Ê†ºÂºèÊ™¢Êü•ÂÆåÊàêÔºå‰∏ÄÂàáÊ≠£Â∏∏ÔºÅ\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "EXPECTED_COLUMNS = [\n",
        "    \"unique_id\", \"gender\", \"hold racket handed\",\n",
        "    \"play years_0\", \"play years_1\", \"play years_2\",\n",
        "    \"level_2\", \"level_3\", \"level_4\", \"level_5\"\n",
        "]\n",
        "csv_path = \"./AICUP/sample_submission.csv\"\n",
        "def check_submission_format(csv_path):\n",
        "    file_path = Path(csv_path)\n",
        "    if not file_path.exists():\n",
        "        print(f\"‚ùå Êâæ‰∏çÂà∞Ê™îÊ°àÔºö{csv_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ ÊâæÂà∞Ê™îÊ°àÔºö{csv_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # === Ê™¢Êü•Ê¨Ñ‰ΩçÂêçÁ®± ===\n",
        "    if list(df.columns) != EXPECTED_COLUMNS:\n",
        "        print(\"‚ùå Ê¨Ñ‰ΩçÂêçÁ®±‰∏çÊ≠£Á¢∫ÔºÅÊáâÁÇ∫Ôºö\")\n",
        "        print(EXPECTED_COLUMNS)\n",
        "        print(\"ÂØ¶ÈöõÊ¨Ñ‰ΩçÔºö\")\n",
        "        print(list(df.columns))\n",
        "        return\n",
        "    print(\"‚úÖ Ê¨Ñ‰ΩçÂêçÁ®±Ê≠£Á¢∫\")\n",
        "\n",
        "    # === Ê™¢Êü•Ê¨Ñ‰ΩçÊï∏Èáè ===\n",
        "    if df.shape[1] != 10:\n",
        "        print(f\"‚ùå Ê¨Ñ‰ΩçÊï∏ÈáèÈåØË™§ÔºåÊáâÁÇ∫ 10 Ê¨ÑÔºåÂØ¶ÈöõÁÇ∫ {df.shape[1]}\")\n",
        "        return\n",
        "    print(\"‚úÖ Ê¨Ñ‰ΩçÊï∏ÈáèÊ≠£Á¢∫\")\n",
        "\n",
        "    # === Ê™¢Êü•Ê¨Ñ‰ΩçÂûãÂà• ===\n",
        "    non_numeric = df.drop(columns=[\"unique_id\"]).select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    if non_numeric:\n",
        "        print(f\"‚ùå ‰ª•‰∏ãÊ¨Ñ‰Ωç‰∏çÊòØÊï∏ÂÄºÂûãÔºö{non_numeric}\")\n",
        "        return\n",
        "    print(\"‚úÖ ÊâÄÊúâÊ©üÁéáÊ¨Ñ‰ΩçÁöÜÁÇ∫Êï∏ÂÄºÂûã\")\n",
        "\n",
        "    # === Ê™¢Êü•Ê©üÁéáÁØÑÂúçÊòØÂê¶Âú® 0ÔΩû1 ===\n",
        "    probs = df.drop(columns=[\"unique_id\"])\n",
        "    if ((probs < 0) | (probs > 1)).any().any():\n",
        "        print(\"‚ùå ÊúâÊ©üÁéáÂÄºË∂ÖÂá∫ 0~1 ÁØÑÂúç\")\n",
        "        rows = probs[(probs < 0) | (probs > 1)].dropna(how='all')\n",
        "        print(\"ÈåØË™§Ê®£Êú¨Ôºö\")\n",
        "        print(rows.head())\n",
        "        return\n",
        "    print(\"‚úÖ ÊâÄÊúâÈ†êÊ∏¨Ê©üÁéáÈÉΩÂú® 0 ~ 1 ÁØÑÂúçÂÖß\")\n",
        "\n",
        "    print(\"üéâ CSV Ê†ºÂºèÊ™¢Êü•ÂÆåÊàêÔºå‰∏ÄÂàáÊ≠£Â∏∏ÔºÅ\")\n",
        "\n",
        "# === Âü∑Ë°åÊ™¢Êü• ===\n",
        "check_submission_format(\"./AICUP/sample_submission.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def FFT(xreal, ximag):    \n",
        "    n = 2\n",
        "    while(n*2 <= len(xreal)):\n",
        "        n *= 2\n",
        "    \n",
        "    p = int(math.log(n, 2))\n",
        "    \n",
        "    for i in range(0, n):\n",
        "        a = i\n",
        "        b = 0\n",
        "        for j in range(0, p):\n",
        "            b = int(b*2 + a%2)\n",
        "            a = a/2\n",
        "        if(b > i):\n",
        "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
        "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
        "            \n",
        "    wreal = []\n",
        "    wimag = []\n",
        "        \n",
        "    arg = float(-2 * math.pi / n)\n",
        "    treal = float(math.cos(arg))\n",
        "    timag = float(math.sin(arg))\n",
        "    \n",
        "    wreal.append(float(1.0))\n",
        "    wimag.append(float(0.0))\n",
        "    \n",
        "    for j in range(1, int(n/2)):\n",
        "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
        "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
        "        \n",
        "    m = 2\n",
        "    while(m < n + 1):\n",
        "        for k in range(0, n, m):\n",
        "            for j in range(0, int(m/2), 1):\n",
        "                index1 = k + j\n",
        "                index2 = int(index1 + m / 2)\n",
        "                t = int(n * j / m)\n",
        "                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
        "                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
        "                ureal = xreal[index1]\n",
        "                uimag = ximag[index1]\n",
        "                xreal[index1] = ureal + treal\n",
        "                ximag[index1] = uimag + timag\n",
        "                xreal[index2] = ureal - treal\n",
        "                ximag[index2] = uimag - timag\n",
        "        m *= 2\n",
        "        \n",
        "    return n, xreal, ximag   \n",
        "    \n",
        "def FFT_data(input_data, swinging_times):   \n",
        "    txtlength = swinging_times[-1] - swinging_times[0]\n",
        "    a_mean = [0] * txtlength\n",
        "    g_mean = [0] * txtlength\n",
        "       \n",
        "    for num in range(len(swinging_times)-1):\n",
        "        a = []\n",
        "        g = []\n",
        "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
        "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
        "\n",
        "        a_mean[num] = (sum(a) / len(a))\n",
        "        g_mean[num] = (sum(a) / len(a))\n",
        "    \n",
        "    return a_mean, g_mean\n",
        "\n",
        "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
        "    allsum = []\n",
        "    mean = []\n",
        "    var = []\n",
        "    rms = []\n",
        "    XYZmean_a = 0\n",
        "    a = []\n",
        "    g = []\n",
        "    a_s1 = 0\n",
        "    a_s2 = 0\n",
        "    g_s1 = 0\n",
        "    g_s2 = 0\n",
        "    a_k1 = 0\n",
        "    a_k2 = 0\n",
        "    g_k1 = 0\n",
        "    g_k2 = 0\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            allsum = input_data[i]\n",
        "            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "            continue\n",
        "        \n",
        "        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "       \n",
        "        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            var = input_data[i]\n",
        "            rms = input_data[i]\n",
        "            continue\n",
        "\n",
        "        var = [var[feature_index] + math.pow((input_data[i][feature_index] - mean[feature_index]), 2) for feature_index in range(len(input_data[i]))]\n",
        "        rms = [rms[feature_index] + math.pow(input_data[i][feature_index], 2) for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    a_max = [max(a)]\n",
        "    a_min = [min(a)]\n",
        "    a_mean = [sum(a) / len(a)]\n",
        "    g_max = [max(g)]\n",
        "    g_min = [min(g)]\n",
        "    g_mean = [sum(g) / len(g)]\n",
        "    \n",
        "    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n",
        "        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n",
        "        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n",
        "        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n",
        "        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n",
        "        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n",
        "    \n",
        "    a_s1 = a_s1 / len(input_data)\n",
        "    a_s2 = a_s2 / len(input_data)\n",
        "    g_s1 = g_s1 / len(input_data)\n",
        "    g_s2 = g_s2 / len(input_data)\n",
        "    a_k2 = math.pow(a_s2, 1.5)\n",
        "    g_k2 = math.pow(g_s2, 1.5)\n",
        "    a_s2 = a_s2 * a_s2\n",
        "    g_s2 = g_s2 * g_s2\n",
        "    \n",
        "    a_kurtosis = [a_s1 / a_s2]\n",
        "    g_kurtosis = [g_s1 / g_s2]\n",
        "    a_skewness = [a_k1 / a_k2]\n",
        "    g_skewness = [g_k1 / g_k2]\n",
        "    \n",
        "    a_fft_mean = 0\n",
        "    g_fft_mean = 0\n",
        "    cut = int(n_fft / swinging_times)\n",
        "    a_psd = []\n",
        "    g_psd = []\n",
        "    entropy_a = []\n",
        "    entropy_g = []\n",
        "    e1 = []\n",
        "    e3 = []\n",
        "    e2 = 0\n",
        "    e4 = 0\n",
        "    \n",
        "    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n",
        "        a_fft_mean += a_fft[i]\n",
        "        g_fft_mean += g_fft[i]\n",
        "        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n",
        "        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n",
        "        e1.append(math.pow(a_psd[-1], 0.5))\n",
        "        e3.append(math.pow(g_psd[-1], 0.5))\n",
        "        \n",
        "    a_fft_mean = a_fft_mean / cut\n",
        "    g_fft_mean = g_fft_mean / cut\n",
        "    \n",
        "    a_psd_mean = sum(a_psd) / len(a_psd)\n",
        "    g_psd_mean = sum(g_psd) / len(g_psd)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        e2 += math.pow(a_psd[i], 0.5)\n",
        "        e4 += math.pow(g_psd[i], 0.5)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n",
        "        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n",
        "    \n",
        "    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n",
        "    g_entropy_mean = sum(entropy_g) / len(entropy_g)       \n",
        "        \n",
        "    \n",
        "    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n",
        "    writer.writerow(output)\n",
        "# === ÊâãÂãïÊåáÂÆö UID ===\n",
        "uid = \"3211\"\n",
        "txt_path = Path(f\"./AICUP/test_data/{uid}.txt\")\n",
        "csv_path = Path(f\"./AICUP/tabular_data_test/{uid}.csv\")\n",
        "\n",
        "# === ËÆÄÂÖ•Ë≥áÊñô ===\n",
        "with open(txt_path) as f:\n",
        "    lines = f.read().splitlines()\n",
        "    data = [list(map(int, l.strip().split())) for l in lines if len(l.strip().split()) == 6]\n",
        "\n",
        "swing_index = np.linspace(0, len(data), 28, dtype=int)\n",
        "headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']\n",
        "\n",
        "# === FFT ËôïÁêÜ ===\n",
        "a_fft, g_fft = FFT_data(data, swing_index)\n",
        "a_fft_imag = [0] * len(a_fft)\n",
        "g_fft_imag = [0] * len(g_fft)\n",
        "n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
        "n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
        "\n",
        "# === ÂØ´ÂÖ•ÂñÆ‰∏ÄÊ™îÊ°à ===\n",
        "with open(csv_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(headerList)\n",
        "    for i in range(1, len(swing_index)):\n",
        "        try:\n",
        "            seg = data[swing_index[i-1]:swing_index[i]]\n",
        "            if len(seg) == 0:\n",
        "                print(f\"‚ö†Ô∏è Á©∫ÊÆµÔºö{uid}_{i-1}\")\n",
        "                continue\n",
        "            feature(seg, i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ÁôºÁîüÈåØË™§Êñº {uid}_{i-1}Ôºö{e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
