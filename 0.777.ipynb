{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# === è¨­å®šè³‡æ–™å¤¾ ===\n",
        "ROOT = Path('./AICUP')\n",
        "TXT_DIR = ROOT / 'train_data'              # æˆ– 'test_data'\n",
        "INFO_CSV = ROOT / 'train_info.csv'         # æˆ– 'test_info.csv'\n",
        "SAVE_DIR = ROOT / 'sequence_data_train'    # æˆ– 'sequence_data_test'\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === è¼‰å…¥ info.csv ä¸¦è™•ç† cut_point ===\n",
        "info = pd.read_csv(INFO_CSV)\n",
        "\n",
        "def fix_cut_point_format(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    try:\n",
        "        return [int(n) for n in re.findall(r'\\d+', str(x))]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "info['cut_point'] = info['cut_point'].apply(fix_cut_point_format)\n",
        "info = info[info['cut_point'].apply(lambda x: len(x) >= 2)]  # è‡³å°‘èƒ½åˆ‡å‡º 1 æ®µ\n",
        "\n",
        "# === è®€å– txt æˆ numpy array ===\n",
        "def read_txt_as_array(txt_path):\n",
        "    lines = Path(txt_path).read_text().splitlines()\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 6:\n",
        "            try:\n",
        "                data.append([int(x) for x in parts])\n",
        "            except:\n",
        "                continue\n",
        "    return np.array(data)\n",
        "\n",
        "# === æ”¶é›†æ‰€æœ‰æ®µè½ï¼Œå¾ŒçºŒçµ±ä¸€å»ºç«‹ scaler ===\n",
        "all_segments = []\n",
        "meta = []\n",
        "\n",
        "# === åˆ‡å‰²æ¯å€‹ txt æª”æ¡ˆ ===\n",
        "for _, row in tqdm(info.iterrows(), total=len(info)):\n",
        "    uid = row['unique_id']\n",
        "    cut_points = row['cut_point']\n",
        "    txt_file = TXT_DIR / f\"{uid}.txt\"\n",
        "\n",
        "    if not txt_file.exists():\n",
        "        continue\n",
        "\n",
        "    raw = read_txt_as_array(txt_file)\n",
        "\n",
        "    for i in range(len(cut_points) - 1):\n",
        "        start, end = cut_points[i], cut_points[i + 1]\n",
        "        segment = raw[start:end]\n",
        "\n",
        "        if segment.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # è£œ 0 åˆ° (85, 6)\n",
        "        if segment.shape[0] < 85:\n",
        "            pad_len = 85 - segment.shape[0]\n",
        "            pad = np.zeros((pad_len, 6))\n",
        "            segment = np.vstack([segment, pad])\n",
        "        elif segment.shape[0] > 85:\n",
        "            segment = segment[:85]\n",
        "\n",
        "        if segment.shape != (85, 6):\n",
        "            continue\n",
        "\n",
        "        all_segments.append(segment)\n",
        "        meta.append((uid, i))\n",
        "\n",
        "# === çµ±ä¸€å»º scalerï¼Œfit æ‰€æœ‰ segment å¾Œå„²å­˜ ===\n",
        "print(f'Number of segments collected: {len(all_segments)}')\n",
        "all_data = np.concatenate(all_segments, axis=0)  # (N*85, 6)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(all_data)\n",
        "joblib.dump(scaler, ROOT / \"scaler.pkl\")\n",
        "print(\"âœ… Scaler å·²å„²å­˜åˆ°ï¼š\", ROOT / \"scaler.pkl\")\n",
        "\n",
        "# === æ­£è¦åŒ–å¾Œå†å„²å­˜æ¯ç­† segment ===\n",
        "for segment, (uid, i) in tqdm(zip(all_segments, meta), total=len(all_segments)):\n",
        "    normed = scaler.transform(segment)\n",
        "    np.save(SAVE_DIR / f\"{uid}_{i}.npy\", normed)\n",
        "\n",
        "print(f\"âœ… å…±å„²å­˜ {len(all_segments)} ç­†æ­£è¦åŒ–å¾Œçš„ (85,6) .npy è‡³ï¼š{SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "# === è·¯å¾‘è¨­å®š ===\n",
        "ROOT = Path('./AICUP')\n",
        "TXT_DIR = ROOT / 'test_data'\n",
        "INFO_CSV = ROOT / 'test_info.csv'\n",
        "SAVE_DIR = ROOT / 'sequence_data_test'\n",
        "SCALER_PATH = ROOT / 'scaler.pkl'\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === è¼‰å…¥ cut_point ===\n",
        "info = pd.read_csv(INFO_CSV)\n",
        "\n",
        "def fix_cut_point_format(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    return [int(n) for n in re.findall(r'\\d+', str(x))]\n",
        "\n",
        "info['cut_point'] = info['cut_point'].apply(fix_cut_point_format)\n",
        "info = info[info['cut_point'].apply(lambda x: len(x) >= 2)]\n",
        "\n",
        "# === è¼‰å…¥è¨“ç·´ç”¨çš„ StandardScaler ===\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "\n",
        "# === txt to array ===\n",
        "def read_txt_as_array(txt_path):\n",
        "    lines = Path(txt_path).read_text().splitlines()\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 6:\n",
        "            try:\n",
        "                data.append([int(x) for x in parts])\n",
        "            except:\n",
        "                continue\n",
        "    return np.array(data)\n",
        "\n",
        "# === é–‹å§‹è™•ç†æ¯å€‹æª”æ¡ˆ ===\n",
        "count = 0\n",
        "for _, row in tqdm(info.iterrows(), total=len(info), desc=\"ğŸ“‚ è™•ç†æ¸¬è©¦æª”æ¡ˆ\"):\n",
        "    uid = row['unique_id']\n",
        "    cut_points = row['cut_point']\n",
        "    uid_int = int(float(uid))\n",
        "    txt_file = TXT_DIR / f\"{uid_int}.txt\"\n",
        "    \n",
        "    if not txt_file.exists():\n",
        "        print(f\"[âŒ æª”æ¡ˆä¸å­˜åœ¨] {txt_file}\")\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"[âœ… æª”æ¡ˆæ‰¾åˆ°] {txt_file}\")\n",
        "\n",
        "\n",
        "    if not txt_file.exists():\n",
        "        continue\n",
        "\n",
        "    raw = read_txt_as_array(txt_file)\n",
        "\n",
        "    for i in range(len(cut_points) - 1):\n",
        "        start, end = cut_points[i], cut_points[i + 1]\n",
        "        segment = raw[start:end]\n",
        "\n",
        "        # è£œ 0 åˆ° (85, 6)\n",
        "        if segment.shape[0] < 85:\n",
        "            pad = np.zeros((85 - segment.shape[0], 6))\n",
        "            segment = np.vstack([segment, pad])\n",
        "        elif segment.shape[0] > 85:\n",
        "            segment = segment[:85]\n",
        "\n",
        "        if segment.shape != (85, 6):\n",
        "            continue\n",
        "\n",
        "        # ä½¿ç”¨ scaler åš Z-score æ­£è¦åŒ–\n",
        "        normed = scaler.transform(segment)\n",
        "\n",
        "        # å„²å­˜ç‚º npy\n",
        "        np.save(SAVE_DIR / f\"{uid}_{i}.npy\", normed)\n",
        "        count += 1\n",
        "\n",
        "print(f\"\\nâœ… å…±å„²å­˜ {count} ç­†æ¸¬è©¦è³‡æ–™åˆ°ï¼š{SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡æ¨™ç±¤æª”æ¡ˆè·¯å¾‘ ===\n",
        "DATA_DIR = Path(\"./AICUP/sequence_data_train\")\n",
        "INFO_CSV = Path(\"./AICUP/train_info.csv\")\n",
        "\n",
        "# === è®€å– train_info.csv ===\n",
        "info = pd.read_csv(INFO_CSV)\n",
        "\n",
        "# === æ”¶é›†æ¯ç­† npy è³‡æ–™èˆ‡æ¨™ç±¤ ===\n",
        "from tqdm import tqdm\n",
        "\n",
        "X, y_gender, y_handed, y_years, y_level, groups = [], [], [], [], [], []\n",
        "\n",
        "for i, row in tqdm(info.iterrows(), total=len(info), desc=\"è®€å– npy\"):\n",
        "    uid = row['unique_id']\n",
        "    pid = row['player_id']\n",
        "    for seg_id in range(27):\n",
        "        npy_path = DATA_DIR / f\"{uid}_{seg_id}.npy\"\n",
        "        if not npy_path.exists():\n",
        "            continue\n",
        "        data = np.load(npy_path)\n",
        "        if data.shape != (85, 6):\n",
        "            continue\n",
        "        X.append(data)\n",
        "        y_gender.append(row['gender'])\n",
        "        y_handed.append(row['hold racket handed'])\n",
        "        y_years.append(row['play years'])\n",
        "        y_level.append(row['level'])\n",
        "        groups.append(pid)\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# === æ¨™ç±¤ç·¨ç¢¼ ===\n",
        "le_gender = LabelEncoder(); y_gender = le_gender.fit_transform(y_gender)\n",
        "le_handed = LabelEncoder(); y_handed = le_handed.fit_transform(y_handed)\n",
        "le_years = LabelEncoder(); y_years = le_years.fit_transform(y_years)\n",
        "le_level = LabelEncoder(); y_level = le_level.fit_transform(y_level)\n",
        "\n",
        "# === åˆ†è¨“ç·´/é©—è­‰é›†ï¼ˆä¾ player_idï¼‰===\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(X, groups=groups))\n",
        "\n",
        "X_train, X_val = X[train_idx], X[val_idx]\n",
        "y_gender_train, y_gender_val = y_gender[train_idx], y_gender[val_idx]\n",
        "y_handed_train, y_handed_val = y_handed[train_idx], y_handed[val_idx]\n",
        "y_years_train, y_years_val = y_years[train_idx], y_years[val_idx]\n",
        "y_level_train, y_level_val = y_level[train_idx], y_level[val_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def FFT(xreal, ximag):    \n",
        "    n = 2\n",
        "    while(n*2 <= len(xreal)):\n",
        "        n *= 2\n",
        "    \n",
        "    p = int(math.log(n, 2))\n",
        "    \n",
        "    for i in range(0, n):\n",
        "        a = i\n",
        "        b = 0\n",
        "        for j in range(0, p):\n",
        "            b = int(b*2 + a%2)\n",
        "            a = a/2\n",
        "        if(b > i):\n",
        "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
        "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
        "            \n",
        "    wreal = []\n",
        "    wimag = []\n",
        "        \n",
        "    arg = float(-2 * math.pi / n)\n",
        "    treal = float(math.cos(arg))\n",
        "    timag = float(math.sin(arg))\n",
        "    \n",
        "    wreal.append(float(1.0))\n",
        "    wimag.append(float(0.0))\n",
        "    \n",
        "    for j in range(1, int(n/2)):\n",
        "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
        "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
        "        \n",
        "    m = 2\n",
        "    while(m < n + 1):\n",
        "        for k in range(0, n, m):\n",
        "            for j in range(0, int(m/2), 1):\n",
        "                index1 = k + j\n",
        "                index2 = int(index1 + m / 2)\n",
        "                t = int(n * j / m)\n",
        "                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
        "                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
        "                ureal = xreal[index1]\n",
        "                uimag = ximag[index1]\n",
        "                xreal[index1] = ureal + treal\n",
        "                ximag[index1] = uimag + timag\n",
        "                xreal[index2] = ureal - treal\n",
        "                ximag[index2] = uimag - timag\n",
        "        m *= 2\n",
        "        \n",
        "    return n, xreal, ximag   \n",
        "    \n",
        "def FFT_data(input_data, swinging_times):   \n",
        "    txtlength = swinging_times[-1] - swinging_times[0]\n",
        "    a_mean = [0] * txtlength\n",
        "    g_mean = [0] * txtlength\n",
        "       \n",
        "    for num in range(len(swinging_times)-1):\n",
        "        a = []\n",
        "        g = []\n",
        "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
        "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
        "\n",
        "        a_mean[num] = (sum(a) / len(a))\n",
        "        g_mean[num] = (sum(a) / len(a))\n",
        "    \n",
        "    return a_mean, g_mean\n",
        "\n",
        "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
        "    allsum = []\n",
        "    mean = []\n",
        "    var = []\n",
        "    rms = []\n",
        "    XYZmean_a = 0\n",
        "    a = []\n",
        "    g = []\n",
        "    a_s1 = 0\n",
        "    a_s2 = 0\n",
        "    g_s1 = 0\n",
        "    g_s2 = 0\n",
        "    a_k1 = 0\n",
        "    a_k2 = 0\n",
        "    g_k1 = 0\n",
        "    g_k2 = 0\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            allsum = input_data[i]\n",
        "            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "            continue\n",
        "        \n",
        "        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "       \n",
        "        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            var = input_data[i]\n",
        "            rms = input_data[i]\n",
        "            continue\n",
        "\n",
        "        var = [var[feature_index] + math.pow((input_data[i][feature_index] - mean[feature_index]), 2) for feature_index in range(len(input_data[i]))]\n",
        "        rms = [rms[feature_index] + math.pow(input_data[i][feature_index], 2) for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    a_max = [max(a)]\n",
        "    a_min = [min(a)]\n",
        "    a_mean = [sum(a) / len(a)]\n",
        "    g_max = [max(g)]\n",
        "    g_min = [min(g)]\n",
        "    g_mean = [sum(g) / len(g)]\n",
        "    \n",
        "    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n",
        "        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n",
        "        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n",
        "        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n",
        "        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n",
        "        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n",
        "    \n",
        "    a_s1 = a_s1 / len(input_data)\n",
        "    a_s2 = a_s2 / len(input_data)\n",
        "    g_s1 = g_s1 / len(input_data)\n",
        "    g_s2 = g_s2 / len(input_data)\n",
        "    a_k2 = math.pow(a_s2, 1.5)\n",
        "    g_k2 = math.pow(g_s2, 1.5)\n",
        "    a_s2 = a_s2 * a_s2\n",
        "    g_s2 = g_s2 * g_s2\n",
        "    \n",
        "    a_kurtosis = [a_s1 / a_s2]\n",
        "    g_kurtosis = [g_s1 / g_s2]\n",
        "    a_skewness = [a_k1 / a_k2]\n",
        "    g_skewness = [g_k1 / g_k2]\n",
        "    \n",
        "    a_fft_mean = 0\n",
        "    g_fft_mean = 0\n",
        "    cut = int(n_fft / swinging_times)\n",
        "    a_psd = []\n",
        "    g_psd = []\n",
        "    entropy_a = []\n",
        "    entropy_g = []\n",
        "    e1 = []\n",
        "    e3 = []\n",
        "    e2 = 0\n",
        "    e4 = 0\n",
        "    \n",
        "    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n",
        "        a_fft_mean += a_fft[i]\n",
        "        g_fft_mean += g_fft[i]\n",
        "        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n",
        "        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n",
        "        e1.append(math.pow(a_psd[-1], 0.5))\n",
        "        e3.append(math.pow(g_psd[-1], 0.5))\n",
        "        \n",
        "    a_fft_mean = a_fft_mean / cut\n",
        "    g_fft_mean = g_fft_mean / cut\n",
        "    \n",
        "    a_psd_mean = sum(a_psd) / len(a_psd)\n",
        "    g_psd_mean = sum(g_psd) / len(g_psd)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        e2 += math.pow(a_psd[i], 0.5)\n",
        "        e4 += math.pow(g_psd[i], 0.5)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n",
        "        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n",
        "    \n",
        "    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n",
        "    g_entropy_mean = sum(entropy_g) / len(entropy_g)       \n",
        "        \n",
        "    \n",
        "    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n",
        "    writer.writerow(output)\n",
        "\n",
        "def data_generate():\n",
        "    datapath = './AICUP/train_data'\n",
        "    tar_dir = './AICUP/tabular_data_train'\n",
        "    pathlist_txt = Path(datapath).glob('**/*.txt')\n",
        "    os.makedirs(tar_dir, exist_ok=True)\n",
        "    \n",
        "    for file in pathlist_txt:\n",
        "        f = open(file)\n",
        "\n",
        "        All_data = []\n",
        "\n",
        "        count = 0\n",
        "        for line in f.readlines():\n",
        "            if line == '\\n' or count == 0:\n",
        "                count += 1\n",
        "                continue\n",
        "            num = line.split(' ')\n",
        "            if len(num) > 5:\n",
        "                tmp_list = []\n",
        "                for i in range(6):\n",
        "                    tmp_list.append(int(num[i]))\n",
        "                All_data.append(tmp_list)\n",
        "        \n",
        "        f.close()\n",
        "\n",
        "        swing_index = np.linspace(0, len(All_data), 28, dtype = int)\n",
        "        # filename.append(int(Path(file).stem))\n",
        "        # all_swing.append([swing_index])\n",
        "\n",
        "        headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']                \n",
        "        \n",
        "\n",
        "        with open('./{dir}/{fname}.csv'.format(dir = tar_dir, fname = Path(file).stem), 'w', newline = '') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(headerList)\n",
        "            try:\n",
        "                a_fft, g_fft = FFT_data(All_data, swing_index)\n",
        "                a_fft_imag = [0] * len(a_fft)\n",
        "                g_fft_imag = [0] * len(g_fft)\n",
        "                n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
        "                n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
        "                for i in range(len(swing_index)):\n",
        "                    if i==0:\n",
        "                        continue\n",
        "                    feature(All_data[swing_index[i-1]: swing_index[i]], i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
        "            except:\n",
        "                print(Path(file).stem)\n",
        "                continue\n",
        "data_generate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "\n",
        "# === è¨­å®šè·¯å¾‘ ===\n",
        "TABULAR_DIR = Path(\"./AICUP/tabular_data_train\")\n",
        "SCALER_PATH = \"./AICUP/tabular_scaler.pkl\"\n",
        "\n",
        "# === æ”¶é›†æ‰€æœ‰ tabular ç‰¹å¾µ ===\n",
        "tabular_list = []\n",
        "for file in sorted(TABULAR_DIR.glob(\"*.csv\")):\n",
        "    df = pd.read_csv(file)\n",
        "    tabular_list.append(df.values)  # shape: (27, 34)\n",
        "\n",
        "X_all = np.vstack(tabular_list)  # shape: (N, 34)\n",
        "\n",
        "# === å»ºç«‹èˆ‡å„²å­˜ MinMaxScaler ===\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_all)\n",
        "\n",
        "joblib.dump(scaler, SCALER_PATH)\n",
        "print(f\"âœ… Tabular MinMaxScaler å·²å„²å­˜è‡³: {SCALER_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Dropout, Bidirectional, LSTM, GlobalAveragePooling1D, Dense, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import AUC\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "\n",
        "# === åƒæ•¸è¨­å®š ===\n",
        "SEQ_DIR = Path('./AICUP/sequence_data_train')\n",
        "TAB_DIR = Path('./AICUP/tabular_data_train')\n",
        "INFO_CSV = './AICUP/train_info.csv'\n",
        "WINDOW = 40\n",
        "STRIDE = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# === è®€å–è¨“ç·´è³‡è¨Š ===\n",
        "info = pd.read_csv(INFO_CSV).set_index(\"unique_id\")\n",
        "\n",
        "# === ç”¢ç”Ÿ sliding window åˆ‡ç‰‡æ¸…å–® ===\n",
        "samples = []\n",
        "for file in sorted(SEQ_DIR.glob(\"*.npy\")):\n",
        "    uid, seg_id = file.stem.split(\"_\")\n",
        "    uid, seg_id = int(uid), int(seg_id)\n",
        "    if uid not in info.index: continue\n",
        "    samples.append({'uid': uid, 'seg_id': seg_id, 'seq_path': file, 'tab_path': TAB_DIR / f\"{uid}.csv\"})\n",
        "\n",
        "samples_df = pd.DataFrame(samples)\n",
        "\n",
        "# === åˆ†ç¾¤åˆ‡ train/val ===\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(samples_df, groups=samples_df['uid']))\n",
        "train_samples = samples_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_samples = samples_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "# === tabular scaler ===\n",
        "tab_all = []\n",
        "for s in samples:\n",
        "    tab = pd.read_csv(s['tab_path']).values\n",
        "    tab_all.append(tab)\n",
        "tab_all = np.vstack(tab_all)\n",
        "scaler = StandardScaler().fit(tab_all)\n",
        "joblib.dump(scaler, './tabular_scaler.pkl')\n",
        "\n",
        "# === label encoding ===\n",
        "label_encoders = {}\n",
        "for col in ['gender', 'hold racket handed', 'play years', 'level']:\n",
        "    le = LabelEncoder()\n",
        "    info[col] = le.fit_transform(info[col])\n",
        "    label_encoders[col] = le\n",
        "joblib.dump(label_encoders, './label_encoders.pkl')\n",
        "\n",
        "# === è³‡æ–™ç”¢ç”Ÿå™¨ ===\n",
        "class DualInputGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size, scaler, info_df):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.scaler = scaler\n",
        "        self.info_df = info_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_seq, X_tab, y_gender, y_handed, y_years, y_level = [], [], [], [], [], []\n",
        "\n",
        "        for _, row in batch.iterrows():\n",
        "            seq = np.load(row['seq_path'])\n",
        "            if seq.shape[0] < WINDOW:\n",
        "                pad = np.zeros((WINDOW - seq.shape[0], 6))\n",
        "                seq = np.vstack([seq, pad])\n",
        "            elif seq.shape[0] > WINDOW:\n",
        "                seq = seq[:WINDOW]\n",
        "\n",
        "            tab = pd.read_csv(row['tab_path']).iloc[row['seg_id']].values.astype(np.float32)\n",
        "            tab = self.scaler.transform([tab])[0]\n",
        "\n",
        "            label_row = self.info_df.loc[row['uid']]\n",
        "            yg, yh = label_row['gender'], label_row['hold racket handed']\n",
        "            yy, yl = label_row['play years'], label_row['level']\n",
        "\n",
        "            X_seq.append(seq)\n",
        "            X_tab.append(tab)\n",
        "            y_gender.append(yg)\n",
        "            y_handed.append(yh)\n",
        "            y_years.append(yy)\n",
        "            y_level.append(yl)\n",
        "\n",
        "        return (\n",
        "            [np.array(X_seq), np.array(X_tab)],\n",
        "            {\n",
        "                'gender': np.array(y_gender).astype(np.float32),\n",
        "                'handed': np.array(y_handed).astype(np.float32),\n",
        "                'years': to_categorical(y_years, num_classes=3),\n",
        "                'level': to_categorical(y_level, num_classes=4),\n",
        "            }\n",
        "        )\n",
        "\n",
        "train_gen = DualInputGenerator(train_samples, BATCH_SIZE, scaler, info)\n",
        "val_gen = DualInputGenerator(val_samples, BATCH_SIZE, scaler, info)\n",
        "\n",
        "# === å»ºç«‹æ¨¡å‹ ===\n",
        "seq_input = Input(shape=(40, 6))\n",
        "x = Conv1D(64, 3, activation='relu', padding='same')(seq_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "x = LayerNormalization()(x)\n",
        "attn2 = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "x = LayerNormalization()(x + attn2)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "# å¼·åŒ– Tabular åˆ†æ”¯\n",
        "tab_input = Input(shape=(34,))\n",
        "t = Dense(256, activation='relu')(tab_input)\n",
        "t = BatchNormalization()(t)\n",
        "t = Dropout(0.4)(t)\n",
        "t = Dense(128, activation='relu')(t)\n",
        "t = BatchNormalization()(t)\n",
        "t = Dropout(0.4)(t)\n",
        "t = Dense(64, activation='relu')(t)\n",
        "t = Dropout(0.4)(t)\n",
        "\n",
        "merged = concatenate([x, t])\n",
        "merged = Dense(128, activation='relu')(merged)\n",
        "merged = Dropout(0.4)(merged)\n",
        "\n",
        "output_gender = Dense(1, activation='sigmoid', name='gender')(merged)\n",
        "output_handed = Dense(1, activation='sigmoid', name='handed')(merged)\n",
        "output_years = Dense(3, activation='softmax', name='years')(merged)\n",
        "output_level = Dense(4, activation='softmax', name='level')(merged)\n",
        "\n",
        "model = Model(inputs=[seq_input, tab_input], outputs=[output_gender, output_handed, output_years, output_level])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss={\n",
        "        'gender': 'binary_crossentropy',\n",
        "        'handed': 'binary_crossentropy',\n",
        "        'years': 'categorical_crossentropy',\n",
        "        'level': 'categorical_crossentropy'\n",
        "    },\n",
        "    metrics={\n",
        "        'gender': AUC(name='auc'),\n",
        "        'handed': AUC(name='auc'),\n",
        "        'years': AUC(name='auc'),\n",
        "        'level': AUC(name='auc')\n",
        "    }\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# === è¨“ç·´ ===\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_level_auc', patience=7, mode='max', restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_level_auc', factor=0.5, patience=3, verbose=1, mode='max')\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å„²å­˜æ¨¡å‹ï¼ˆå»ºè­°æ ¼å¼ï¼‰\n",
        "model.save(\"./AICUP/model_fusion.keras\")\n",
        "print(\"âœ… æ¨¡å‹å·²å„²å­˜è‡³ ./AICUP/model_fusion.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === è·¯å¾‘è¨­å®š ===\n",
        "MODEL_PATH = \"./AICUP/model_fusion.keras\"\n",
        "SCALER_PATH = \"./tabular_scaler.pkl\"\n",
        "SEQ_DIR = Path(\"./AICUP/sequence_data_test\")\n",
        "TAB_DIR = Path(\"./AICUP/tabular_data_test\")\n",
        "SUBMIT_PATH = \"./AICUP/sample_submission.csv\"\n",
        "WINDOW = 40\n",
        "\n",
        "# === è¼‰å…¥æ¨¡å‹èˆ‡ scaler ===\n",
        "model = load_model(MODEL_PATH, compile=False)\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "# === è£œç©ºç™½çš„å£æ‰æª”æ¡ˆ ===\n",
        "# for f in invalid_files:\n",
        "#     np.save(f, np.zeros((85, 6), dtype=np.float32))\n",
        "# === æ•´ç†æ¯ä½é¸æ‰‹æ‰€æœ‰æ®µè½ ===\n",
        "uid_dict = defaultdict(list)\n",
        "for file in sorted(SEQ_DIR.glob(\"*.npy\")):\n",
        "    uid_str = \"_\".join(file.stem.split(\"_\")[:-1])  # e.g. '1968.0'\n",
        "    uid = int(float(uid_str))  # å…ˆ float å† int\n",
        "    uid_dict[uid].append(file)\n",
        "\n",
        "submit_rows = []\n",
        "\n",
        "for uid in tqdm(sorted(uid_dict.keys()), desc=\"ğŸ§  æ¨è«–ä¸­\"):\n",
        "    segments_seq, segments_tab = [], []\n",
        "\n",
        "    for file in sorted(uid_dict[uid]):\n",
        "        seg_id = int(file.stem.split(\"_\")[-1])\n",
        "        seq = np.load(file, allow_pickle=True)\n",
        "        if seq.shape[0] < WINDOW:\n",
        "            pad = np.zeros((WINDOW - seq.shape[0], 6))\n",
        "            seq = np.vstack([seq, pad])\n",
        "        elif seq.shape[0] > WINDOW:\n",
        "            seq = seq[:WINDOW]\n",
        "        segments_seq.append(seq)\n",
        "\n",
        "        tab_path = TAB_DIR / f\"{uid}.csv\"\n",
        "        if not tab_path.exists():\n",
        "            print(f\"âš ï¸ æ‰¾ä¸åˆ° tabular è³‡æ–™: {tab_path}\")\n",
        "            continue\n",
        "        tab_df = pd.read_csv(tab_path)\n",
        "        if seg_id >= len(tab_df):\n",
        "            print(f\"âš ï¸ è·³é {uid}_{seg_id}ï¼Œtabular ç„¡å°æ‡‰è³‡æ–™\")\n",
        "            continue\n",
        "        tab = tab_df.iloc[seg_id].values.astype(np.float32)\n",
        "        tab = scaler.transform([tab])[0]\n",
        "        segments_tab.append(tab)\n",
        "\n",
        "    if not segments_seq or len(segments_seq) != len(segments_tab):\n",
        "        print(f\"âš ï¸ UID {uid} çš„æ®µæ•¸ä¸ä¸€è‡´ï¼Œè·³é\")\n",
        "        continue\n",
        "\n",
        "    X_seq = np.array(segments_seq)\n",
        "    X_tab = np.array(segments_tab)\n",
        "\n",
        "    preds = model.predict([X_seq, X_tab], verbose=0)\n",
        "    if not isinstance(preds, list):\n",
        "        preds = [preds]\n",
        "\n",
        "    weighted_preds = []\n",
        "    for i, p in enumerate(preds):\n",
        "        if p.shape[1] == 1:\n",
        "            # sigmoidï¼šå…ˆ squeezeï¼Œå†åè½‰ â†’ é æ¸¬ label=1 çš„æ©Ÿç‡ï¼ˆç”·/å³æ‰‹ï¼‰\n",
        "            p = p.squeeze(axis=1)\n",
        "            p = 1.0 - p  # ğŸ‘ˆ åè½‰ï¼šåŸæœ¬æ˜¯ label=0ï¼ˆå¥³/å·¦æ‰‹ï¼‰çš„æ©Ÿç‡\n",
        "            weights = p\n",
        "            weighted_avg = np.average(p, weights=weights)\n",
        "            weighted_preds.append(weighted_avg)\n",
        "        else:\n",
        "            # softmax\n",
        "            weights = np.max(p, axis=1)\n",
        "            weighted_avg = np.average(p, axis=0, weights=weights)\n",
        "            weighted_preds.append(weighted_avg)\n",
        "\n",
        "    row = [\n",
        "        uid,\n",
        "        np.float32(weighted_preds[0]),         # gender â†’ ç”·ç”Ÿæ©Ÿç‡\n",
        "        np.float32(weighted_preds[1]),         # handed â†’ å³æ‰‹æ©Ÿç‡\n",
        "        *map(np.float32, weighted_preds[2]),   # play years softmax: 3 é¡\n",
        "        *map(np.float32, weighted_preds[3])    # level softmax: 4 é¡\n",
        "    ]\n",
        "    submit_rows.append(row)\n",
        "\n",
        "# === è¼¸å‡º CSV ===\n",
        "columns = [\n",
        "    \"unique_id\", \"gender\", \"hold racket handed\",\n",
        "    \"play years_0\", \"play years_1\", \"play years_2\",\n",
        "    \"level_2\", \"level_3\", \"level_4\", \"level_5\"\n",
        "]\n",
        "df_submit = pd.DataFrame(submit_rows, columns=columns)\n",
        "df_submit = df_submit.sort_values(\"unique_id\")\n",
        "float_cols = df_submit.columns.difference([\"unique_id\"])\n",
        "df_submit[float_cols] = df_submit[float_cols].astype(np.float32)\n",
        "df_submit.to_csv(SUBMIT_PATH, index=False, float_format=\"%.4f\")\n",
        "\n",
        "print(f\"\\nâœ… å·²å„²å­˜é æ¸¬çµæœè‡³: {SUBMIT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "invalid_files = []\n",
        "\n",
        "for file in sorted(Path(\"AICUP/sequence_data_test\").glob(\"*.npy\")):\n",
        "    try:\n",
        "        data = np.load(file, allow_pickle=False)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ç„¡æ³•è®€å–ï¼š{file.name}ï¼ŒåŸå› ï¼š{e}\")\n",
        "        invalid_files.append(file)\n",
        "\n",
        "print(f\"\\nå…±æ‰¾åˆ° {len(invalid_files)} å€‹ç„¡æ³•è®€å–çš„æª”æ¡ˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "EXPECTED_COLUMNS = [\n",
        "    \"unique_id\", \"gender\", \"hold racket handed\",\n",
        "    \"play years_0\", \"play years_1\", \"play years_2\",\n",
        "    \"level_2\", \"level_3\", \"level_4\", \"level_5\"\n",
        "]\n",
        "csv_path = \"./AICUP/sample_submission.csv\"\n",
        "def check_submission_format(csv_path):\n",
        "    file_path = Path(csv_path)\n",
        "    if not file_path.exists():\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{csv_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ… æ‰¾åˆ°æª”æ¡ˆï¼š{csv_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # === æª¢æŸ¥æ¬„ä½åç¨± ===\n",
        "    if list(df.columns) != EXPECTED_COLUMNS:\n",
        "        print(\"âŒ æ¬„ä½åç¨±ä¸æ­£ç¢ºï¼æ‡‰ç‚ºï¼š\")\n",
        "        print(EXPECTED_COLUMNS)\n",
        "        print(\"å¯¦éš›æ¬„ä½ï¼š\")\n",
        "        print(list(df.columns))\n",
        "        return\n",
        "    print(\"âœ… æ¬„ä½åç¨±æ­£ç¢º\")\n",
        "\n",
        "    # === æª¢æŸ¥æ¬„ä½æ•¸é‡ ===\n",
        "    if df.shape[1] != 10:\n",
        "        print(f\"âŒ æ¬„ä½æ•¸é‡éŒ¯èª¤ï¼Œæ‡‰ç‚º 10 æ¬„ï¼Œå¯¦éš›ç‚º {df.shape[1]}\")\n",
        "        return\n",
        "    print(\"âœ… æ¬„ä½æ•¸é‡æ­£ç¢º\")\n",
        "\n",
        "    # === æª¢æŸ¥æ¬„ä½å‹åˆ¥ ===\n",
        "    non_numeric = df.drop(columns=[\"unique_id\"]).select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    if non_numeric:\n",
        "        print(f\"âŒ ä»¥ä¸‹æ¬„ä½ä¸æ˜¯æ•¸å€¼å‹ï¼š{non_numeric}\")\n",
        "        return\n",
        "    print(\"âœ… æ‰€æœ‰æ©Ÿç‡æ¬„ä½çš†ç‚ºæ•¸å€¼å‹\")\n",
        "\n",
        "    # === æª¢æŸ¥æ©Ÿç‡ç¯„åœæ˜¯å¦åœ¨ 0ï½1 ===\n",
        "    probs = df.drop(columns=[\"unique_id\"])\n",
        "    if ((probs < 0) | (probs > 1)).any().any():\n",
        "        print(\"âŒ æœ‰æ©Ÿç‡å€¼è¶…å‡º 0~1 ç¯„åœ\")\n",
        "        rows = probs[(probs < 0) | (probs > 1)].dropna(how='all')\n",
        "        print(\"éŒ¯èª¤æ¨£æœ¬ï¼š\")\n",
        "        print(rows.head())\n",
        "        return\n",
        "    print(\"âœ… æ‰€æœ‰é æ¸¬æ©Ÿç‡éƒ½åœ¨ 0 ~ 1 ç¯„åœå…§\")\n",
        "\n",
        "    print(\"ğŸ‰ CSV æ ¼å¼æª¢æŸ¥å®Œæˆï¼Œä¸€åˆ‡æ­£å¸¸ï¼\")\n",
        "\n",
        "# === åŸ·è¡Œæª¢æŸ¥ ===\n",
        "check_submission_format(\"./AICUP/sample_submission.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def FFT(xreal, ximag):    \n",
        "    n = 2\n",
        "    while(n*2 <= len(xreal)):\n",
        "        n *= 2\n",
        "    \n",
        "    p = int(math.log(n, 2))\n",
        "    \n",
        "    for i in range(0, n):\n",
        "        a = i\n",
        "        b = 0\n",
        "        for j in range(0, p):\n",
        "            b = int(b*2 + a%2)\n",
        "            a = a/2\n",
        "        if(b > i):\n",
        "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
        "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
        "            \n",
        "    wreal = []\n",
        "    wimag = []\n",
        "        \n",
        "    arg = float(-2 * math.pi / n)\n",
        "    treal = float(math.cos(arg))\n",
        "    timag = float(math.sin(arg))\n",
        "    \n",
        "    wreal.append(float(1.0))\n",
        "    wimag.append(float(0.0))\n",
        "    \n",
        "    for j in range(1, int(n/2)):\n",
        "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
        "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
        "        \n",
        "    m = 2\n",
        "    while(m < n + 1):\n",
        "        for k in range(0, n, m):\n",
        "            for j in range(0, int(m/2), 1):\n",
        "                index1 = k + j\n",
        "                index2 = int(index1 + m / 2)\n",
        "                t = int(n * j / m)\n",
        "                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
        "                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
        "                ureal = xreal[index1]\n",
        "                uimag = ximag[index1]\n",
        "                xreal[index1] = ureal + treal\n",
        "                ximag[index1] = uimag + timag\n",
        "                xreal[index2] = ureal - treal\n",
        "                ximag[index2] = uimag - timag\n",
        "        m *= 2\n",
        "        \n",
        "    return n, xreal, ximag   \n",
        "    \n",
        "def FFT_data(input_data, swinging_times):   \n",
        "    txtlength = swinging_times[-1] - swinging_times[0]\n",
        "    a_mean = [0] * txtlength\n",
        "    g_mean = [0] * txtlength\n",
        "       \n",
        "    for num in range(len(swinging_times)-1):\n",
        "        a = []\n",
        "        g = []\n",
        "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
        "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
        "\n",
        "        a_mean[num] = (sum(a) / len(a))\n",
        "        g_mean[num] = (sum(a) / len(a))\n",
        "    \n",
        "    return a_mean, g_mean\n",
        "\n",
        "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
        "    allsum = []\n",
        "    mean = []\n",
        "    var = []\n",
        "    rms = []\n",
        "    XYZmean_a = 0\n",
        "    a = []\n",
        "    g = []\n",
        "    a_s1 = 0\n",
        "    a_s2 = 0\n",
        "    g_s1 = 0\n",
        "    g_s2 = 0\n",
        "    a_k1 = 0\n",
        "    a_k2 = 0\n",
        "    g_k1 = 0\n",
        "    g_k2 = 0\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            allsum = input_data[i]\n",
        "            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "            continue\n",
        "        \n",
        "        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
        "        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
        "       \n",
        "        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        if i==0:\n",
        "            var = input_data[i]\n",
        "            rms = input_data[i]\n",
        "            continue\n",
        "\n",
        "        var = [var[feature_index] + math.pow((input_data[i][feature_index] - mean[feature_index]), 2) for feature_index in range(len(input_data[i]))]\n",
        "        rms = [rms[feature_index] + math.pow(input_data[i][feature_index], 2) for feature_index in range(len(input_data[i]))]\n",
        "        \n",
        "    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
        "    \n",
        "    a_max = [max(a)]\n",
        "    a_min = [min(a)]\n",
        "    a_mean = [sum(a) / len(a)]\n",
        "    g_max = [max(g)]\n",
        "    g_min = [min(g)]\n",
        "    g_mean = [sum(g) / len(g)]\n",
        "    \n",
        "    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n",
        "    \n",
        "    for i in range(len(input_data)):\n",
        "        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n",
        "        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n",
        "        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n",
        "        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n",
        "        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n",
        "        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n",
        "    \n",
        "    a_s1 = a_s1 / len(input_data)\n",
        "    a_s2 = a_s2 / len(input_data)\n",
        "    g_s1 = g_s1 / len(input_data)\n",
        "    g_s2 = g_s2 / len(input_data)\n",
        "    a_k2 = math.pow(a_s2, 1.5)\n",
        "    g_k2 = math.pow(g_s2, 1.5)\n",
        "    a_s2 = a_s2 * a_s2\n",
        "    g_s2 = g_s2 * g_s2\n",
        "    \n",
        "    a_kurtosis = [a_s1 / a_s2]\n",
        "    g_kurtosis = [g_s1 / g_s2]\n",
        "    a_skewness = [a_k1 / a_k2]\n",
        "    g_skewness = [g_k1 / g_k2]\n",
        "    \n",
        "    a_fft_mean = 0\n",
        "    g_fft_mean = 0\n",
        "    cut = int(n_fft / swinging_times)\n",
        "    a_psd = []\n",
        "    g_psd = []\n",
        "    entropy_a = []\n",
        "    entropy_g = []\n",
        "    e1 = []\n",
        "    e3 = []\n",
        "    e2 = 0\n",
        "    e4 = 0\n",
        "    \n",
        "    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n",
        "        a_fft_mean += a_fft[i]\n",
        "        g_fft_mean += g_fft[i]\n",
        "        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n",
        "        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n",
        "        e1.append(math.pow(a_psd[-1], 0.5))\n",
        "        e3.append(math.pow(g_psd[-1], 0.5))\n",
        "        \n",
        "    a_fft_mean = a_fft_mean / cut\n",
        "    g_fft_mean = g_fft_mean / cut\n",
        "    \n",
        "    a_psd_mean = sum(a_psd) / len(a_psd)\n",
        "    g_psd_mean = sum(g_psd) / len(g_psd)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        e2 += math.pow(a_psd[i], 0.5)\n",
        "        e4 += math.pow(g_psd[i], 0.5)\n",
        "    \n",
        "    for i in range(cut):\n",
        "        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n",
        "        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n",
        "    \n",
        "    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n",
        "    g_entropy_mean = sum(entropy_g) / len(entropy_g)       \n",
        "        \n",
        "    \n",
        "    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n",
        "    writer.writerow(output)\n",
        "# === æ‰‹å‹•æŒ‡å®š UID ===\n",
        "uid = \"3211\"\n",
        "txt_path = Path(f\"./AICUP/test_data/{uid}.txt\")\n",
        "csv_path = Path(f\"./AICUP/tabular_data_test/{uid}.csv\")\n",
        "\n",
        "# === è®€å…¥è³‡æ–™ ===\n",
        "with open(txt_path) as f:\n",
        "    lines = f.read().splitlines()\n",
        "    data = [list(map(int, l.strip().split())) for l in lines if len(l.strip().split()) == 6]\n",
        "\n",
        "swing_index = np.linspace(0, len(data), 28, dtype=int)\n",
        "headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']\n",
        "\n",
        "# === FFT è™•ç† ===\n",
        "a_fft, g_fft = FFT_data(data, swing_index)\n",
        "a_fft_imag = [0] * len(a_fft)\n",
        "g_fft_imag = [0] * len(g_fft)\n",
        "n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
        "n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
        "\n",
        "# === å¯«å…¥å–®ä¸€æª”æ¡ˆ ===\n",
        "with open(csv_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(headerList)\n",
        "    for i in range(1, len(swing_index)):\n",
        "        try:\n",
        "            seg = data[swing_index[i-1]:swing_index[i]]\n",
        "            if len(seg) == 0:\n",
        "                print(f\"âš ï¸ ç©ºæ®µï¼š{uid}_{i-1}\")\n",
        "                continue\n",
        "            feature(seg, i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤æ–¼ {uid}_{i-1}ï¼š{e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
